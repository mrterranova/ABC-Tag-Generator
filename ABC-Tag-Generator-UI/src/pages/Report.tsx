import { Link } from "react-router-dom";
import "../App.css";

export default function Report() {

  return (
    <>
      <div className="parallax-layer">
        <img
          className="parallax-layer-i"
          src="open-book.jpg"
          alt="Bookshelf"
        />
        </div>
        <h1 className="report-header">Section 1: Introduction – Why I Chose This Capstone </h1>
          <p className="report-para">I chose this capstone project because I was interested in building a system that could help categorize written content in a way that could eventually support a fast and accurate way of quickly categorizing lessons and short stories that are being typed and eventually placed online. While my original interest was centered on organizing lessons and moral themes that have been passed down in my family for generations, those materials are not always available in large, structured, or easily accessible datasets. As a result, they are difficult to use directly in a machine learning project that requires a significant amount of text data for this capstone project.</p>
          <p className="report-para">To address this limitation, I used book descriptions as a practical substitute. Book summaries are widely available, formatted with consistancy, and provide a large volume of text suitable for training and evaluating machine learning models. Although these descriptions are not the lesson plans which I hope to eventually translate this project into, books often contain similar underlying themes, values, and ideas. This makes them an effective stand-in for the type of content I ultimately hope to organize and categorize.</p>
          <p className="report-para">By working with book descriptions, this capstone allows me to explore automated tag generation and text categorization using real-world data while maintaining a clear connection to my original goal. In addition, this approach naturally supports the development of a recommendation system, where books can be grouped or suggested based on shared themes or descriptions. Overall, this project balances practical data availability with a meaningful long-term application, making it a strong and realistic foundation for my capstone work.</p>
            <p className="resource-link"><a target="_blank" href="https://docs.google.com/document/d/1LA4ByN4rLHVYERVa4I6C5dJ2M48CMZ1-EXuci5CXbPo/edit?usp=sharing">Click here to view my capstone proposal.</a></p>

        <h1 className="report-header">Section 2: Research – What I Researched for This Capstone</h1>
          <p className="report-para">For this capstone project, I researched different methods for analyzing and categorizing text, with a focus on keyword extraction and text classification. The goal of this research was to understand how written content can be automatically labeled in a way that supports organization, categorization, and can lend itself to future recommendation systems.</p>
          <p className="report-para">I first examined unsupervised keyword extraction methods, specifically YAKE!, to understand how keywords can be generated without labeled training data. This approach helped establish a baseline for understanding features such as word frequency. I then researched supervised text classification using large, structured datasets, which led to studying the PubMed 200k RCT dataset. This research highlighted how labeled data and dataset size affect model performance and classification accuracy inside scientific journal, which is not far off from my project concerning book descriptions.</p>
          <p className="report-para">Finally, I explored deep learning approaches for keyword extraction, particularly BERT-based models. This research focused on how contextual embeddings improve the model's ability to capture meaning in text compared to statistical methods. It also led to my overall choice in models later in the capstone as I worked. Seeing and running BERT locally proved to be effective, accurate, and powerful enough for what I wanted to achieve in my capstone. Together, all these approaches provided a broad understanding of the trade-offs between simplicity and accuracy, and helped guide the design of my capstone project.</p>
          <p className="report-para">Later during fine tuning I would also look at two more repositories of similar projects to compare results with my results. This proved to be very useful in concluding in how my final model held up to other realworld comparisons.</p>
            <p className="resource-link"><a className="resource-link" target="_blank" href="https://docs.google.com/document/d/1yZqcMTKxlm1_XMROkPmYzhPf2dHGH0yPbO4kRG1Kmd4/edit?usp=sharing">Click here to view my research prior to starting my capstone.</a></p>
            <p className="resource-link">YAKE! </p><p className="resource-link"><a target="_blank" href="https://liaad.github.io/yake/docs/--home">YAKE! Keyword extraction from single documents using multiple local features (Campos et al., 2020)</a><a target="_blank" href="https://github.com/LIAAD/yake">    || Github Repository </a></p>
            <p className="resource-link">PubMed 200k RCT dataset </p><p className="resource-link"><a href="https://arxiv.org/abs/1710.06071">PubMed 200k RCT: a dataset for sequential sentence classification (Dernoncourt, 2017)</a><a target="_blank" href=" https://github.com/Franck-Dernoncourt/pubmed-rct">    ||  Github Repository</a></p>
            <p className="resource-link">BERT-based Keyphrase Extraction</p><p className="resource-link"><a href="https://arxiv.org/pdf/1910.05786"></a>Progress Notes Classification and Keyword Extraction using Attention-based Deep Learning Models with BERT (Tang et al., 2019)<a target="_blank" href="https://github.com/mrterranova/Capstone-Project-Research">    || Substitute Github Repository(Official repository could not be located)</a></p>
            <p className="resource-link"><a target="_blank" href="https://github.com/mrterranova/Automated-Book-Tagging-Generator/blob/main/documents/Capstone8Recap.md">Click to view my research done in the fine tuning phase of my project</a></p>
            <p className="resource-link"><a target="_blank" href="https://github.com/akshaybhatia10/Book-Genre-Classification">"Book Genre Classification" Repository</a></p>
            <p className="resource-link"><a target="_blank" href="https://github.com/obaidtambo/books_genre_predictor">"Book Genre Preditior" Repository</a></p>


        <h1 className="report-header">Section 3: Data - My Resources for This Capstone</h1>
            <p className="re-img-container"><img className="resource-img" src="googlebooks-resource.png" alt="google-books img" /><img className="resource-img" src="goodreads-resource.png" alt="goodreads img"/><img className="resource-img" src="bookkaggle-resource.png" alt="kaggle img"/></p>
          <p className="report-para">For this project, I compiled a large book dataset by combining multiple sources to ensure broad coverage and diversity among all categories. The primary sources included the Google Books API, filtered by a wide swath of categories that could lend itself easily to one generalized category. And the remaining two datasets were large publicly available Kaggle datasets: the Books Dataset by Abdallah Wagih Ibrahim and the Goodreads 100k books dataset. The combined dataset originally contained over 137,000 records, well above the capstone minimum amount of 15,000. This provided me with enough room to do the cleaning necessary to ensure that categories were balanced and records with missing data could be extracted from the dataset list without concern.</p>
          <p className="report-para">Extensive cleaning and normalization were applied, including removal of duplicate entries, filtering for English-language content, standardizing authors and categories, and ensuring meaningful descriptions. At this stage, 15 major categories were identified, including art, business/finance, culture/folklore, fantasy, general fiction, history, humanities, psychology, religion, romance, science, science fiction, slice of life, spirituality, and thriller. This would later be reduced down to the 9 categories seen in the project. In order to acheive a limited number of categories while maintaining an average of ~4,500 samples per category, some liberties were taken. The data's subgenres, or the original genre(s), was filtered in order to ensure that all data fell into one category or another. Due to the nature of some of the data being multi-genre, there were some inaccuracies in this process leading to discrepancies seen later in the model. However, the tradeoff was gaining valuable time needed for moving forward with the project. And this tradeoff also included keeping multi-genre books which kept the data samples relatively high.</p>
          <p className="report-para">Outliers found from the description length were left alone as this was real world data and I did not want to cap the results. I did however, remove descriptions that were not verbose enough to produce substantial information needed in the machine model. In the end, ~59,707 book records remained after a thorough cleaning and balancing process and 9 categories had enough data to move forward. All the other categories were dropped for the current version in the model. This is a feature I would prefer to revisit later when time and resources allow. </p>
            <p className="resource-link"><a className="resource-link" target="_blank">Click here to visit the data cleaning required for the success of my project.</a></p>
            <p className="resource-link"><a target="_blank" href="https://developers.google.com/books">Google Books API</a></p>
            <p className="resource-link"><a target="_blank" href="https://www.kaggle.com/datasets/mdhamani/goodreads-books-100k">Goodreads 100k books</a></p>
            <p className="resource-link"><a target="_blank" href="https://www.kaggle.com/datasets/abdallahwagih/books-dataset">the Books Dataset by Abdallah Wagih Ibrahim</a></p>
        
        <h1 className="report-header">Section 4: Baseline - What I Established for This Capstone</h1>
          <p className="report-para">The baseline model, implemented using Logistic Regression with TF-IDF vectorization, was the initial benchmark for classifying book descriptions into 13 categories, and later reduced to 9 categories. The model explored various configurations of n-grams and max feature counts, with the best performing setup using 20,000 features and n-grams from 1 to 3. This configuration achieved an overall accuracy of 59%. Categories such as business/finance, food, romance, and thriller had the highest precision and recall, indicating that descriptions in these categories tend to have language more distinctive than other categories. In contrast, categories like psychology, humanities, and history were harder to classify, likely due to overlapping vocabulary with other categories. The confusion matrix highlighted natural overlaps between closely related categories, particularly fantasy/horror and science fiction, where descriptive words often cross genre boundaries.</p>
          <p className="report-para">Other experimentation involved Random Forest and XGBoost classifiers to determine whether ensemble methods could improve the performance of the dataset based on the model. Random Forest models were tested with varying numbers of estimators (100-300) and n-gram ranges (1-3, 1-5). While tuning parameters slightly improved accuracy, the highest performing Random Forest model achieved 52.3% accuracy, underperforming compared to Logistic Regression. Random Forest struggled most with psychology, humanities, and overlapping categories like fantasy/horror and science fiction, suggesting that the tree-based approach could not fully capture nuanced word relationships in text data.</p>
          <p className="report-para">The XGBoost model, particularly with hyperparameter tuning via RandomizedSearchCV, demonstrated the strongest performance of all models tested. Using 20,000 features with n-grams from 1-5, the tuned XGBoost achieved a macro F1 score of 0.567 and accuracy of 56.7%. While XGBoost improved classification for categories with distinct descriptive language, such as business/finance, food, romance, and thriller, it continued to struggle with psychology and other less distinctive categories. Overall, XGBoost proved slightly more robust than Random Forest, likely due to its gradient boosting mechanism, which better handles subtle differences in word frequency and context across categories.</p>
          <p className="report-para">It was at this part of my journey that the underperforming categories had to be reanalyzed further. Categories such as humanities, psychology, and slice of life were consistently bleeding into other categories. The decision was made at this point to try restrengthening these categories in order to boost performance. However, this came to no avail and eventually the 3 categories were cut in this stage and did not proceed too much further in the experimental phase. The category size was then reduced to 10 categories.</p>
            <p className="re-img-container">
              <figure>
                <img className="re-graph" src="logistic_regression.png" alt="logistic regression" />
                <figcaption>Confusion Matrix for Logistic Regression</figcaption>
              </figure>
              <figure>
                <img className="re-graph" src="random_forest.png" alt="random forest"/>
                <figcaption>Confusion Matrix for Random Forest</figcaption>
              </figure>
              <figure>
                <img className="re-graph" src="xgboost.png" alt="xgboost" />
                <figcaption>Confusion Matrix for XGBoost</figcaption>
              </figure>
            </p>
            <p className="resource-link"><a href="https://github.com/mrterranova/Automated-Book-Tagging-Generator/blob/main/notebooks/Capstone_Experiment_Testing_2.ipynb">Click here to view baseline work and conclusions.</a></p>


        <h1 className="report-header">Section 5: Experimentation - What Models I Tested for This Capstone</h1>
          <p className="report-para">n the experimental phase of this project, I tested with two transformer-based models, BERT and DistilBERT, to classify book descriptions into categories. This allowed me to delve into deep-learning models which had been the end goal since the research and discovery phase of this project. Both models were trained on a carefully curated dataset where ambiguous categories such as psychology and slice of life were eventualy re-evaluated and complete removed from the dataset to improve clarity. The training process involved tokenization of text data, label encoding, and splitting the dataset into stratified training and testing sets. Across multiple runs, BERT and DistilBERT achieved overall accuracies of approximately 72% and macro F1 scores around 0.72-0.73, indicating that the models were able to learn meaningful patterns from the text descriptions rather than relying on random guessing.</p>
          <p className="report-para">Each model has its advantages and limitations. BERT showed slightly higher performance in most categories, particularly those that involve more nuanced semantic understanding, such as art and science. Its ability to capture complex contextual relationships likely contributed to this edge. However, BERT requires more computational resources and longer training times, which can be a limitation in practical applications. On the other hand, DistilBERT, a lighter and faster version of BERT, performed nearly as well in terms of accuracy and F1 score. Its efficiency makes it suitable for scenarios where computational resources are limited or where faster training and inference are desirable. In the end, DistilBERT was less effective than the BERT model at capturing nuanced data and therefore, Bert was selected to move forward.</p>
          <p className="report-para">It was at this time more reading and research had been done on a roBERTa model. Therefore, while continuing to the fine tuning phase of this project, I also experimented with fine-tuning the roBERTa model. The roBERTa model showed even greater success overall in the final phases of selecting the model and concluding the fine tuning aspect.</p>
            <p className="resource-link"><a href="https://github.com/mrterranova/Automated-Book-Tagging-Generator/blob/main/notebooks/Capstone_Experiment_Testing_2.ipynb">Click here to view experimenting with models.</a></p>

        <h1 className="report-header">Section 6: Fine Tuning - Wrapping Up Model Performance for This Capstone</h1>
          <p className="report-para">In this capstone, BERT and RoBERTa were evaluated for book genre classification using titles, authors, and descriptions across ten categories. Both models achieved strong predictive performance, with overall accuracies around 74-75% and comparable precision, recall, and F1 scores. BERT showed an edge in narrative-driven categories such as Thriller and Fantasy/Horror, while RoBERTa performed slightly better in factual genres like Business, Food, and Science. The most notable difference lies in training efficiency: BERT required three epochs and over two hours to train, whereas RoBERTa completed training in approximately 97 minutes over two epochs without sacrificing accuracy. Considering the balance of performance and efficiency, RoBERTa presents the more practical choice for this classification task. Future improvements could explore additional metadata, such as extended reviews or book content, to further enhance classification and potentially support a more sophisticated recommendation system.</p>
          <p className="report-para">The final conclusion of the which model and how to tune brought data back to discussion. While my model was on parr with other models as seen above with similar projects, the data for the fantasy/horror was holding back the model from making greater strides concerning accuracy and overall performance. After consulting with mentors on this particular detail, the only conclusion was to rerun how I separated my data and split the fields fantasy/horror, remove horror and add fantasy to science fiction since there was a significant overlap between the two generes, thus making 9 categories to train. This significantly improved my model's results from an accuracy of 74%-75% to 78%.</p>
            <p className="re-img-container">
              <figure>
                <img className="re-graph" src="roberta-final.png" alt="roberta confusion matrix" />
                <figcaption>RoBERTa Confusion Matrix</figcaption>
              </figure>
              <figure>
                <img className="re-graph" src="roberta-matrix.png" alt="roberta classification report"/>
                <figcaption>RoBERTa Classification Report</figcaption>
              </figure>
            </p>
            <p className="resource-link"><a href="https://github.com/mrterranova/Automated-Book-Tagging-Generator/blob/main/notebooks/Capstone_Fine_Tuning.ipynb">Click here to view fine-tuning of BERT and roBERTa models.</a></p>

        <h1 className="report-header">Section 7: Final Model Analysis - Completing the Final Model for This Capstone</h1>
          <p className="report-para">The final roBERTa-based classification model demonstrated strong performance and validated many design decisions made throughout the project. Combining book titles, author names, and descriptions into a single text input proved to be an effective way to give the model enough context for accurate genre classification. The final model achieved a weighted F1-score of approximately 0.78 across nine genres, with especially strong performance in clearly defined categories such as Romance, Food, and Business/Finance. These results suggest that the transformer-based architecture was well suited for capturing meaningful semantic patterns in book metadata, and that balancing the dataset helped maintain consistent performance across all classes.</p>
          <p className="report-para">However, several areas for improvement have been identified through evaluation and the project has been redone from beginning to final model multiple times due to the need for improvement. The model struggled with genres that exhibit significant semantic overlap, such as Science versus Fantasy/Science Fiction and Art versus History, leading to higher confusion rates in these categories. Validation loss trends also suggested there was mild overfitting after multiple training epochs, indicating that additional regularization, and diverse training data may be helpful in the future. While the model performed well overall, improving interpretability and prediction confidence would help build greater trust for end users. These limitations could be addressed through better data with strictly one category in the dataset, clearer labeling strategies, and continued experimentation with alternative model architectures or fine-tuning approaches. Together, these improvements generate a clear path toward making the model better suited for real-world use.</p>

        <h1 className="report-header">Section 8: User Interface - How I developed the UI for This Capstone</h1>
          <p className="report-para">The user interface for this Capstone project was developed using React.js with TypeScript, chosen for its component-based architecture, support for scalable front-end development, and ease of the developer as I am most familiar with react architecture. The UI was designed to be minimal with a focus on the basic usability of the website, allowing users to easily input book information such as title and author, view automatically generated descriptions and tags, and submit or edit data as needed. The automated description retrieval comes from the google api as was used to collect the data in this project. This provides the user faster, multi-optional paths in order to populate the necessary fields to generate category tags for his or her books. Reusable components, including navigation elements and form components, were implemented to ensure consistency across the application and simplify any future enhancements that might develop beyond this course. CSS was used to control layout and responsiveness, with design decisions such as sticky headers and structured form layouts improving user experience and accessibility.</p>
          <p className="report-para">Special attention was given to integrating the UI with backend services and machine learning functionality. The interface communicates with backend APIs to retrieve book metadata and model-generated tags. This enables real-time updates within the form fields. React hooks such as useState and useEffect were used to manage data fetching and state updates, ensuring the user has a positive and seemless experience throughout the website. The UI design gives some clarity and feedback, providing loading states and error handling to inform users during data retrieval or submission. The user interface serves as an effective way for the user to use the underlying machine-learning book tagging model with ease and effectivity.</p>
        
        <h1 className="report-header">Section 9: Deployment - How I deployed This Capstone</h1>
          <p className="report-para">The deployment of this capstone project involved a multi-platform approach to ensure reliability, accessibility, and cost-effectiveness while supporting both the machine learning components and the user-facing application. Each part of the system was deployed using services best suited to its technical requirements, allowing the application to function seamlessly in a production-like environment.</p>
          <p className="report-para">The machine learning component, including the trained RoBERTa model and its supporting Flask-based API, was deployed to Hugging Face Spaces. Hugging Face was selected due to its strong support for machine learning workflows, ease of model hosting, and native integration with Python-based inference services. The model was containerized to ensure consistency across environments, with all dependencies explicitly defined to prevent runtime issues. The Flask API exposes RESTful endpoints that allow external applications to submit text inputs and receive real-time book category predictions. This setup enables scalable and reproducible model inference while keeping the machine learning logic isolated from the rest of the application.</p>
          <p className="report-para">The backend application responsible for request handling and data persistence was developed using Node.js and deployed to Render. Render was chosen primarily for its free-tier support, making it a practical option for a personal capstone project while still offering reliable hosting for server-side applications. The backend integrates an SQLite database to store application data, including user interactions and classification results. Due to the need for persistent data storage, this component could not be deployed to Netlify, which is optimized for static content and does not support server-side databases.</p>
          <p className="report-para">The front-end of the application was built using React and deployed on Netlify, a platform optimized for hosting static web applications. Netlify's seamless integration with GitHub enabled continuous deployment, ensuring that updates pushed to the main branch were automatically re-deployed on the live site. Environment variables were securely managed to protect sensitive configuration details such as API endpoints and access keys, allowing the front-end to communicate safely with both the Node backend and the Hugging Face inference API.</p>
          <p className="report-para">Overall, this deployment strategy resulted in a modular, scalable, and maintainable architecture. By separating concerns across multiple platforms—Hugging Face for machine learning inference, Render for backend services and data storage, and Netlify for the user interface—the capstone project achieves a balance between performance, cost efficiency, and future extensibility. This approach also allows individual components to be independently updated or scaled as the project evolves.</p>

        <h1 className="report-header">Section 10: Conclusion - Final Thoughts on This Capstone</h1>
          <p className="report-para">This capstone project was a valuable opportunity to bring together machine learning, software engineering, and design a cohesive system from beginning to end. Developing an automated book categorizing application challenged me to think beyond model accuracy and consider how data flows through an end-to-end and see the entire process a machine learning engineer would take from design to deployment. Working with a transformer-based model such as roBERTa assisted my understanding of modern NLP techniques and models and gave me a strong introduction with deep learning that will most assuredly be used in future projects. Although there are clear areas for future improvement—such as refining category boundaries from the data level up and expanding the dataset—the project successfully met its goals and provided meaningful insight into deploying machine learning solutions in practical, real-world application. Overall, this capstone represents both a technical achievement and a learning experience that has strengthened my confidence in designing and implementing intelligent systems and making my decisions more sound for future projects in machine learning and AI.</p>
          <p className="resource-link"><Link className="resource-link" to="/next-steps">Please also see my Next Steps sections for where I should go from here.</Link></p>
    </>
  );
}
